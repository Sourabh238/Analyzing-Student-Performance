{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics as stat\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import train_test_split\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.set_color_codes(\"pastel\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be our transformed dataframes\n",
    "mathTransformed = pd.read_csv('Datasets/math_transformed.csv')\n",
    "portTransformed = pd.read_csv('Datasets/port_transformed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "1 school - student's school (binary: 'GP' - Gabriel Pereira or 'MS' - Mousinho da Silveira) \n",
    "2 sex - student's sex (binary: 'F' - female or 'M' - male) \n",
    "3 age - student's age (numeric: from 15 to 22) \n",
    "4 address - student's home address type (binary: 'U' - urban or 'R' - rural) \n",
    "5 famsize - family size (binary: 'LE3' - less or equal to 3 or 'GT3' - greater than 3) \n",
    "6 Pstatus - parent's cohabitation status (binary: 'T' - living together or 'A' - apart) \n",
    "7 Medu - mother's education (numeric: 0 - none, 1 - primary education (4th grade), 2 â€“ 5th to 9th grade, 3 â€“ secondary education or 4 â€“ higher education) \n",
    "8 Fedu - father's education (numeric: 0 - none, 1 - primary education (4th grade), 2 â€“ 5th to 9th grade, 3 â€“ secondary education or 4 â€“ higher education) \n",
    "9 Mjob - mother's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other') \n",
    "10 Fjob - father's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other') \n",
    "11 reason - reason to choose this school (nominal: close to 'home', school 'reputation', 'course' preference or 'other') \n",
    "12 guardian - student's guardian (nominal: 'mother', 'father' or 'other') \n",
    "13 traveltime - home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour) \n",
    "14 studytime - weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours) \n",
    "15 failures - number of past class failures (numeric: n if 1<=n<3, else 4) \n",
    "16 schoolsup - extra educational support (binary: yes or no) \n",
    "17 famsup - family educational support (binary: yes or no) \n",
    "18 paid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no) \n",
    "19 activities - extra-curricular activities (binary: yes or no) \n",
    "20 nursery - attended nursery school (binary: yes or no) \n",
    "21 higher - wants to take higher education (binary: yes or no) \n",
    "22 internet - Internet access at home (binary: yes or no) \n",
    "23 romantic - with a romantic relationship (binary: yes or no) \n",
    "24 famrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent) \n",
    "25 freetime - free time after school (numeric: from 1 - very low to 5 - very high) \n",
    "26 goout - going out with friends (numeric: from 1 - very low to 5 - very high) \n",
    "27 Dalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high) \n",
    "28 Walc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high) \n",
    "29 health - current health status (numeric: from 1 - very bad to 5 - very good) \n",
    "30 absences - number of school absences (numeric: from 0 to 93) \n",
    "\n",
    "# these grades are related with the course subject, Math or Portuguese: \n",
    "31 G1 - first period grade (numeric: from 0 to 20) \n",
    "31 G2 - second period grade (numeric: from 0 to 20) \n",
    "32 G3 - final grade (numeric: from 0 to 20, output target\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in list(mathTransformed):\n",
    "    if not str(mathTransformed[i][0]).isdigit() or not str(portTransformed[i][0]).isdigit():\n",
    "        assert(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mathTransformed['diff'] = mathTransformed['G3'] - mathTransformed['G1']\n",
    "portTransformed['diff'] = portTransformed['G3'] - portTransformed['G1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have finally one labeled / one hot vector encoded our input data and can now start learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "targetColumn = ['diff']\n",
    "featureColumns = [i for i in list(portTransformed) if i not in targetColumn]\n",
    "featureColumnsG1 = [i for i in featureColumns if i not in ['G2', 'G3']]\n",
    "featureColumnsG1G2 = [i for i in featureColumns if i not in ['G3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linearRegression(originalDf, targetColumn, featureColumns):\n",
    "    '''\n",
    "    Runs unregularized linear regression and returns the \n",
    "    R^2 and mean squared error (averaged over all folds \n",
    "    via cross validation)\n",
    "    '''\n",
    "    lm = linear_model.LinearRegression(normalize=True)\n",
    "    X = originalDf[featureColumns]\n",
    "    y = originalDf[targetColumn]\n",
    "    scores = cross_validate(lm, X, y, scoring=['r2','neg_mean_squared_error', 'neg_mean_absolute_error'], cv=10, return_train_score=False)\n",
    "    return lm, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linearRegressionRidge(originalDf, targetColumn, featureColumns):\n",
    "    '''\n",
    "    Runs ridge regularized linear regression and returns the \n",
    "    mean and standard deviation of test scores\n",
    "    '''\n",
    "    parameters = {'alpha' : np.arange(0.0001, 0.01, 0.01)}\n",
    "    scoringMethods = ['r2','neg_mean_squared_error', 'neg_mean_absolute_error']\n",
    "    test_scores = []\n",
    "    lm = linear_model.Ridge(normalize=True)\n",
    "    for score in scoringMethods:\n",
    "        X = originalDf[featureColumns]\n",
    "        y = originalDf[targetColumn]\n",
    "        clf = GridSearchCV(lm, parameters, cv=10, scoring=score)\n",
    "        clf.fit(X,y)\n",
    "        test_scores.append(clf.best_score_)\n",
    "    return clf.best_estimator_, test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linearRegressionLasso(originalDf, targetColumn, featureColumns):\n",
    "    '''\n",
    "    Runs lasso regularized linear regression and returns the \n",
    "    mean and standard deviation of test scores\n",
    "    '''\n",
    "    parameters =  {'alpha' : np.arange(0.0001, 0.1, 0.01)}\n",
    "    scoringMethods = ['r2','neg_mean_squared_error', 'neg_mean_absolute_error']\n",
    "    test_scores = []\n",
    "    lm = linear_model.Lasso(normalize=True)\n",
    "    for score in scoringMethods:\n",
    "        X = originalDf[featureColumns]\n",
    "        y = originalDf[targetColumn]\n",
    "        clf = GridSearchCV(lm, parameters, cv=10, scoring=score)\n",
    "        clf.fit(X,y)\n",
    "        test_scores.append(clf.best_score_)\n",
    "    return clf.best_estimator_, test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def returnAttributesWithLargestCoefficients(training, testing, model):\n",
    "    '''\n",
    "    This methods takes in a model and a training testing. It fits\n",
    "    the model and then returns the attributes with their corresponding\n",
    "    coefficient, ordered from largest to smallest.\n",
    "    '''\n",
    "    model.fit(training, testing)\n",
    "    coefficients = model.coef_[0]\n",
    "    finalLst = []\n",
    "    attributes = list(training)\n",
    "    assert(len(attributes) == len(coefficients))\n",
    "    for i in range(len(attributes)):\n",
    "        finalLst.append([attributes[i], coefficients[i]])\n",
    "    finalLst.sort(key = lambda x: abs(x[1]), reverse=True)\n",
    "    return finalLst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's first start with linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runLinearRegression(df, targetColumn, featureColumns):\n",
    "    '''\n",
    "    This function runs all the linear regression models on the specific\n",
    "    dataset using specified target and feature columns and returns back\n",
    "    the scores as a list of tuples (a,b, c) where a corresponds to the model, \n",
    "    b corresponds to the R^2 scores and c corresponds to the mean squared errors\n",
    "    (over all runs).\n",
    "    '''\n",
    "    unregularized_lm, unregularized = linearRegression(df, targetColumn, featureColumns)\n",
    "    ridge_lm, ridge = linearRegressionRidge(df, targetColumn, featureColumns)\n",
    "    lasso_lm, lasso = linearRegressionLasso(df, targetColumn, featureColumns)\n",
    "    return [[unregularized_lm, unregularized['test_r2'], unregularized['test_neg_mean_squared_error'], unregularized['test_neg_mean_absolute_error']],\n",
    "            [ridge_lm, ridge[0], ridge[1], ridge[2]],\n",
    "            [lasso_lm, lasso[0], lasso[1], lasso[2]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def return_residuals(model, df, col_to_predict, bad_columns):\n",
    "    '''\n",
    "    Using a 90/10 split of data, returns list of residuals for predictions\n",
    "    of testing set.\n",
    "    '''\n",
    "    all_columns = list(df)\n",
    "    all_X = [i for i in all_columns if i not in bad_columns]\n",
    "    y = df[col_to_predict]\n",
    "    X = df[all_X]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "    model.fit(X_train, y_train)\n",
    "    all_residuals = []\n",
    "    residuals = model.predict(X_test)\n",
    "    y_test = list(y_test)\n",
    "    for i in range(len(y_test)):\n",
    "        residual = (y_test[i] - residuals[i])\n",
    "        all_residuals.append(residual)\n",
    "    return all_residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Portuguese \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Predicting G3-G1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unregularized, ridge, lasso = runLinearRegression(portTransformed, targetColumn, featureColumnsG1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unregularized accuracy: -0.16 (+/- 0.45)\n",
      "Unregularized accuracy: -1.81 (+/- 1.03)\n",
      "Unregularized accuracy: -1.02 (+/- 0.27)\n",
      "Ridge accuracy: -0.16 (+/- 0.00)\n",
      "Ridge accuracy: -1.81 (+/- 0.00)\n",
      "Ridge accuracy: -1.02 (+/- 0.00)\n",
      "Lasso accuracy: -0.16 (+/- 0.00)\n",
      "Lasso accuracy: -1.80 (+/- 0.00)\n",
      "Lasso accuracy: -1.02 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "print(\"Unregularized accuracy: %0.2f (+/- %0.2f)\" % (unregularized[1].mean(), unregularized[1].std() * 2))\n",
    "print(\"Unregularized accuracy: %0.2f (+/- %0.2f)\" % (unregularized[2].mean(), unregularized[2].std() * 2))\n",
    "print(\"Unregularized accuracy: %0.2f (+/- %0.2f)\" % (unregularized[3].mean(), unregularized[3].std() * 2))\n",
    "print(\"Ridge accuracy: %0.2f (+/- %0.2f)\" % (ridge[1].mean(), ridge[1].std() * 2))\n",
    "print(\"Ridge accuracy: %0.2f (+/- %0.2f)\" % (ridge[2].mean(), ridge[2].std() * 2))\n",
    "print(\"Ridge accuracy: %0.2f (+/- %0.2f)\" % (ridge[3].mean(), ridge[3].std() * 2))\n",
    "print(\"Lasso accuracy: %0.2f (+/- %0.2f)\" % (lasso[1].mean(), lasso[1].std() * 2))\n",
    "print(\"Lasso accuracy: %0.2f (+/- %0.2f)\" % (lasso[2].mean(), lasso[2].std() * 2))\n",
    "print(\"Lasso accuracy: %0.2f (+/- %0.2f)\" % (lasso[3].mean(), lasso[3].std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "portDiff_lm = return_residuals(lasso[0], portTransformed, 'diff', ['G3', 'G2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Predicting G3-G1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unregularized, ridge, lasso = runLinearRegression(mathTransformed, targetColumn, featureColumnsG1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unregularized accuracy: -0.17 (+/- 0.45)\n",
      "Unregularized accuracy: -2.39 (+/- 1.56)\n",
      "Unregularized accuracy: -1.25 (+/- 0.43)\n",
      "Ridge accuracy: -0.17 (+/- 0.00)\n",
      "Ridge accuracy: -2.40 (+/- 0.00)\n",
      "Ridge accuracy: -1.26 (+/- 0.00)\n",
      "Lasso accuracy: -0.11 (+/- 0.00)\n",
      "Lasso accuracy: -2.29 (+/- 0.00)\n",
      "Lasso accuracy: -1.21 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "print(\"Unregularized accuracy: %0.2f (+/- %0.2f)\" % (unregularized[1].mean(), unregularized[1].std() * 2))\n",
    "print(\"Unregularized accuracy: %0.2f (+/- %0.2f)\" % (unregularized[2].mean(), unregularized[2].std() * 2))\n",
    "print(\"Unregularized accuracy: %0.2f (+/- %0.2f)\" % (unregularized[3].mean(), unregularized[3].std() * 2))\n",
    "print(\"Ridge accuracy: %0.2f (+/- %0.2f)\" % (ridge[1].mean(), ridge[1].std() * 2))\n",
    "print(\"Ridge accuracy: %0.2f (+/- %0.2f)\" % (ridge[2].mean(), ridge[2].std() * 2))\n",
    "print(\"Ridge accuracy: %0.2f (+/- %0.2f)\" % (ridge[3].mean(), ridge[3].std() * 2))\n",
    "print(\"Lasso accuracy: %0.2f (+/- %0.2f)\" % (lasso[1].mean(), lasso[1].std() * 2))\n",
    "print(\"Lasso accuracy: %0.2f (+/- %0.2f)\" % (lasso[2].mean(), lasso[2].std() * 2))\n",
    "print(\"Lasso accuracy: %0.2f (+/- %0.2f)\" % (lasso[3].mean(), lasso[3].std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mathDiff_lm = return_residuals(lasso[0], mathTransformed, 'diff', ['G3', 'G2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's now do SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SVM(originalDf, targetColumn, featureColumns):\n",
    "    '''\n",
    "    Runs ridge regularized linear regression and returns the \n",
    "    mean and standard deviation of test scores\n",
    "    '''\n",
    "    parameters = {'C' : np.arange(0.1, 10, .5)}\n",
    "    test_scores = []\n",
    "    X = originalDf[featureColumns]\n",
    "    y = np.ravel(originalDf[targetColumn])\n",
    "    svr = SVR()\n",
    "    scoringMethods = ['r2','neg_mean_squared_error', 'neg_mean_absolute_error']\n",
    "    for score in scoringMethods:\n",
    "        clf = GridSearchCV(svr, parameters, cv=10, scoring=score)\n",
    "        clf.fit(X,y)\n",
    "        test_scores.append(clf.best_score_)\n",
    "    return clf.best_estimator_, test_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Portuguese \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predicting G3-G1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM accuracy: -0.08 (+/- 0.00)\n",
      "SVM accuracy: -1.72 (+/- 0.00)\n",
      "SVM accuracy: -0.99 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "port_svm_best, results = SVM(portTransformed, targetColumn, featureColumnsG1)\n",
    "print(\"SVM accuracy: %0.2f (+/- %0.2f)\" % (results[0].mean(), results[0].std() * 2))\n",
    "print(\"SVM accuracy: %0.2f (+/- %0.2f)\" % (results[1].mean(), results[1].std() * 2))\n",
    "print(\"SVM accuracy: %0.2f (+/- %0.2f)\" % (results[2].mean(), results[2].std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "portDiff_svm = return_residuals(port_svm_best, portTransformed, 'diff', ['G3', 'G2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predicting G3-G1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM accuracy: -0.05 (+/- 0.00)\n",
      "SVM accuracy: -2.17 (+/- 0.00)\n",
      "SVM accuracy: -1.17 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "math_svm_best, results_math_svm = SVM(mathTransformed, targetColumn, featureColumnsG1)\n",
    "print(\"SVM accuracy: %0.2f (+/- %0.2f)\" % (results_math_svm[0].mean(), results_math_svm[0].std() * 2))\n",
    "print(\"SVM accuracy: %0.2f (+/- %0.2f)\" % (results_math_svm[1].mean(), results_math_svm[1].std() * 2))\n",
    "print(\"SVM accuracy: %0.2f (+/- %0.2f)\" % (results_math_svm[2].mean(), results_math_svm[2].std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mathDiff_svm = return_residuals(math_svm_best, mathTransformed, 'diff', ['G3', 'G2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NAIVE METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculateMAE(lst1, lst2):\n",
    "    ''' \n",
    "    Calculates the mean absolute error between two lists\n",
    "    '''\n",
    "    assert(len(lst1) == len(lst2))\n",
    "    totalError = 0.0\n",
    "    for i in range(len(lst1)):\n",
    "        totalError += abs(lst2[i] - lst1[i])\n",
    "    return totalError / len(lst1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculateMSE(lst1, lst2):\n",
    "    ''' \n",
    "    Calculates the mean squared error between two lists\n",
    "    '''\n",
    "    assert(len(lst1) == len(lst2))\n",
    "    totalError = 0.0\n",
    "    for i in range(len(lst1)):\n",
    "        totalError += (lst2[i] - lst1[i]) ** 2\n",
    "    return totalError / len(lst1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "actualDiffMath = list(mathTransformed['diff'])\n",
    "actualDiffPort = list(portTransformed['diff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_math = sum(actualDiffMath) / len(actualDiffMath)\n",
    "mean_port = sum(actualDiffPort)/ len(actualDiffPort)\n",
    "median_math = stat.median(actualDiffMath)\n",
    "median_port = stat.median(actualDiffPort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_math_lst = [mean_math for _ in range(len(actualDiffMath))]\n",
    "median_math_lst = [median_math for _ in range(len(actualDiffMath))]\n",
    "mean_port_lst = [mean_port for _ in range(len(actualDiffPort))]\n",
    "median_port_lst = [median_port for _ in range (len(actualDiffPort))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for mean, math 1.22294504086\n",
      "MAE for median, math 1.18481012658\n",
      "MAE for mean, port 1.05757583671\n",
      "MAE for median, port 1.03235747304\n"
     ]
    }
   ],
   "source": [
    "print(\"MAE for mean, math \" + str(calculateMAE(mean_math_lst, actualDiffMath)))\n",
    "print(\"MAE for median, math \" + str(calculateMAE(median_math_lst, actualDiffMath)))\n",
    "print(\"MAE for mean, port \" + str(calculateMAE(mean_port_lst, actualDiffPort)))\n",
    "print(\"MAE for median, port \" + str(calculateMAE(median_port_lst, actualDiffPort)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for mean, math 2.32302515622\n",
      "MSE for median, math 2.35443037975\n",
      "MSE for mean, port 1.82326727619\n",
      "MSE for median, port 1.94453004622\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE for mean, math \" + str(calculateMSE(mean_math_lst, actualDiffMath)))\n",
    "print(\"MSE for median, math \" + str(calculateMSE(median_math_lst, actualDiffMath)))\n",
    "print(\"MSE for mean, port \" + str(calculateMSE(mean_port_lst, actualDiffPort)))\n",
    "print(\"MSE for median, port \" + str(calculateMSE(median_port_lst, actualDiffPort)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
